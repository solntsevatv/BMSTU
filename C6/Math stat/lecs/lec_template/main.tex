\title{Лекции по математической статистике}
\chapter{Лекция 5}
\section{Интервальный статистический ряд}
Выше было понятие статистического ряда. Однако, если объем достаточно велик (n > 50), то элементы выборки группируют в так называемый интервальный статистический ряд. Для этого отрезок $J = [x_{(1)}, x_{(n)}]$ разбивают на m равновеликих промежутков. Ширина каждого из них $\Delta = \frac{|J|}{m} = \frac{x_{(1)} - x_{(n)}}{n}$. Данные промежутки строятся по следующему правилу:\\
$J_{i} = [x_{(1)} + (i - 1)\Delta; x_{(i)} + i\Delta), i=\overline{1, m-1}$\\
$J_{m} = [x_{(1)} + (m - 1)\Delta; x_{(n)}]$\\
\begin{figure}[H]
	\center{\includegraphics[scale=0.7]{5_1}}
\end{figure}

Определение интервального статистического ряда, отвечающего выборке $x$ называется таблица следующего вида:\\

\begin{figure}[H]
	\center{\includegraphics[scale=0.7]{5_2}}
\end{figure}

$n_{i}$ - число элементов выблоки $\overrightarrow{x}$, попавших в промежуток $J_{i}, i = \overline{1, m}$\\
Замечание:\\
1) $\sum\limits_{i = 1}^{m} n_{i} = n$\\
2) Для выбора m используют формулу:\\
$m = [log_{2}n] + 2$\\
или\\
$m = [log_{2}n] + 1$\\

\section{Эмпирическая плотность}
Пусть для данной выборки $\overrightarrow{x}$ построен интервальный статистический ряд $(J_{i}, n_{i}), i = \overline{1, m}$\\
Определение:\\
Эмпирической плотностью распределения соответствующей выборки $\overrightarrow{x}$ называется функция:\\
\begin{equation}
f_{n}(x) = 
\begin{cases}
\frac{n_{i}}{n \cdot \Delta}, x \in J_{i}\\
0
\end{cases}
\end{equation}

Замечание:
1) Очевидно, что $\int\limits_{-\infty}^{+\infty} f_{n} (x) dx = \int\limits_{x_{(1)}}^{x_{(m)}} f_{n} (x) dx = \sum\limits_{i=1}^{m}\frac{n_{i}}{n \cdot \Delta} \Delta = 1$\\
Таким образом эмпирическая плотность распределения удовлетворяет условию нормировки. Легко показать, что она обладает всеми свойствами функции плотности распределения.\\
2) $f_{n}(x)$ является кусочно-постоянной функцией:
\begin{figure}[H]
	\center{\includegraphics[scale=0.7]{5_3}}
\end{figure}

3) Функция $f_{n}(x)$ вяляется статистическим аналогом функции плотности распределения вероятности. Доказательство - аналогично доказанному выше результату для функции распределения. $\hat{F}_{n}(x) \overrightarrow{x \rightarrow \infty} F(x)$ на P\\

$f_{n}(x)$ примерно равна $f(x)$ при n >> 1.

Опредениение - график эмпирической функции плотности называется гистограммой.

\section{Полигон частот}
Определение полигона частот - пусть для некоторой выборки $\overrightarrow{x}$ построены гистограммы, по определению полигоном частот называется ломаная, звенья которой соединяют середины верних сторон соседних прямоугольников гистограммы.
\begin{figure}[H]
	\center{\includegraphics[scale=0.7]{5_4}}
\end{figure}

\section{Некоторые распределения, используемые в математической статистике}
\subsection{Гамма-функция Эйлера}
По определению гамма-функцией Эйлера называется выражение $\Gamma: R^{+} \rightarrow R$, определённое правилом:\\
$\Gamma(x) = \int\limits_{0}^{+\infty}e^{-t} t^{x-1}dt$\\

Замечание:\\
1) Интерграл является несобственным первого рода при $x \geqslant 1$;\\
при $x \in (0; 1)$ этот интеграл является несобственным и имеет следующие особенности: в t = 0 - подинтегральная функция имеет разрыв второго рода, верхний предел равен бесконечности. Легко проверить, что данный интеграл сходится при $x > 0$, при остальных вещественных x он расходится.\\

Некоторые свойства гамма-функции:\\
1. $\Gamma(x)$ - является бесконечное число раз дифференцируемой функцией, при этом её к-ая производная задаётся следующей формулой:\\
$\Gamma^{k}(x) = \int\limits_{0}^{+\infty} e^{-t} t^{x - 1} (ln t)^{k} dt$\\
2. $\Gamma(x + 1) = x\Gamma(x), x > 0$\\
3. $\Gamma(1) = 1$\\
4. $\Gamma(n + 1) = n!, n \in N$, по этой причине часто говорят, что гамма-функция является обобщением понятия факториала на вещественные числа.\\
5. $\Gamma(\frac{1}{2}) = \sqrt{\pi}$, вывод через интеграл Пуассона.
6. $\Gamma(\frac{n+1}{2}) = \bigg| $по второму свойству$\bigg| = \frac{n-1}{2}\Gamma(\frac{n-1}{2}) = ... = \frac{n-1}{2} \frac{n-2}{2} ...  \frac{1}{2} \Gamma(\frac{n-1}{2}) = \frac{1 \cdot 3 \cdot 5 ... \cdot (n - 1)}{2^{n}}\sqrt{\pi}$\\
7. Эскиз графика $\Gamma(x)$\\
\begin{figure}[H]
	\center{\includegraphics[scale=0.7]{5_5}}
\end{figure}

\subsection{Гамма-распределение}
Определение: говорят, что случайная величина $\xi$ имеет гамма-распределение, ели её функция плотности распределения вероятности имеет вид:\\
\begin{equation}
f_{\xi}(x) = 
\begin{cases}
\frac{\lambda^{\alpha}}{\Gamma(\alpha)} x^{\alpha - 1} e^{-\lambda x}, x  > 0\\

\end{cases}
\end{equation}
Обозначаеся как $\xi ~ \Gamma(\lambda, \alpha)$

Замечание:\\
1) Экспоненциальное распределениe:\\
\begin{equation}
f_{\xi}(x) = 
\begin{cases}
\lambda e^{-\lambda x}, x  > 0\\
0
\end{cases}
\end{equation}
$Exp(\lambda) = \Gamma(\lambda, 1)$\\

Теорема:\\
Пусть случайная величина $\xi_{1} ~ \Gamma(\lambda, \alpha_{1})$, а $\xi_{1} ~ \Gamma(\lambda, \alpha_{1})$, $\xi_{1}$ и $\xi_{2}$ - независимы. Тогда:\\
$\xi_{1} + \xi_{2} ~ \Gamma(\lambda, \alpha_{1} + \alpha_{2})$\\

Следствие:\\
Если случайные величины $\xi_{1}, \xi_{2}, ..., \xi_{n}$ независимы, причём $\xi_{i} ~ \Gamma(\lambda, \alpha_{i}), i = \overline{1, n}$, то:\\
$\xi_{1} + ... + \xi_{n} ~ \Gamma(\lambda, \alpha_{1} + ... + \alpha_{n})$\\

\subsection{Распределение Релея}
Пусть $\xi ~ \mathcal{N}(0, \sigma^{2})$\\
Говорят, что случайная величина $\xi$ имеет распределения Релея с параметром $\sigma$.

Замечание:\\
1) Несложно показать, что:\\
\begin{equation}
f_{y}(x) = 
\begin{cases}
\frac{1}{\sigma\sqrt{2\pi x}}e^{\frac{-x}{2b^{2}}}, x > 0\\
0
\end{cases}
\end{equation}

2) Распределение Релея является частным случаем гамма-распределения для $\lambda = \frac{1}{2\sigma^{2}}$ и $\lambda = \frac{1}{2}$, то есть $\nu ~ \Gamma(\frac{1}{2\sigma^{2}}, \frac{1}{2})$\\

\subsection{Распределение хи-квадрат}
Пусть:\\
Если случайные величины $\xi_{1}, \xi_{2}, ..., \xi_{n}$ независимы, $\xi_{i} ~ N(0, 1), i = \overline{1, n}$, $\nu = \xi_{1}^{2} + ... + \xi_{n}^{2}$\\

Определение: в этом случае говорят, что случайная величина $\nu$ имеет распределение хи-квадрат с n степенями свободы. Обозначается как $\nu ~ X^{2}(n)$\\

Замечание:\\
1)  $\xi_{i} ~ N(0, 1) \Rightarrow \xi^{2}_{i}$ имеет распределение Релея с параметром $\sigma = 1$, то есть $\xi^{2}_{i} ~ \Gamma(\frac{1}{2}, \frac{1}{2})$. Так как случайные величины $\xi_{1} ... \xi_{n}$ - независимы с учётом свойства гамма-распределения:\\
$\nu = \xi_{1}^{2} + ... + \xi^{2}_{n} ~ \Gamma(\frac{1}{2}, \frac{n}{2})$, то $X^{2} = \Gamma(\frac{1}{2}, \frac{n}{2})$\\
2) Очевидно, что если независимые случайные величины $\nu_{1}, ... \nu_{m}$ имеют распределения $X^{2} (\nu_{i} ~ X^{2} (k_{i}), i=\overline{1, m})$, то $\nu_{1} + ... + \nu_{n} ~ X^{2}(k_{1} + ... k_{m})$\\
3)График функции плотности $\nu ~ X^{2}(n)$\\
\begin{figure}[H]
	\center{\includegraphics[scale=0.7]{5_6}}
\end{figure}

\subsection{Распределение Фишера}
Пусть:\\
1) $\xi_{1}, \xi_{2}$ - независимы
2) $\xi_{i} ~ X^{2}(n_{i}), i = \overline{1,2}$\\
3) $\nu = \frac{n_{1} \xi_{1}}{n_{2} \xi_{2}}$\\

Определение: в этом случае говорят, что случайная величина $\nu$ имеет распределение Фишера со степенями свободы $n_{1} и n_{2}$, $\nu ~ F(n_{1}, n_{2})$\\

Замечания:\\
1) Можно показать, что:\\
\begin{equation}
f_{\nu}(x) = 
\begin{cases}
C \frac{x^{\frac{n_{1}}{2} - 1}}{(1 + \frac{n_{1} x}{n_{2}})^{\frac{n_{1} + n_{2}}{2}}}, x > 0\\
0
\end{cases}
\end{equation}

$C = \frac{(\frac{n_{1}}{n_{2}})^{\frac{n_{1}}{2}}}{B(\frac{n_{1}}{2}, \frac{n_{2}}{2})}$\\
$B(x, y) = \int\limits_{0}^{1} t^{x - 1} (1 - t)^{y - 1} dt$  - бета-функция Эйлера.\\
2) Если $\nu ~ F(n_{1}, n_{2})$, то $\frac{1}{\nu} ~ F(n_{2}, n_{1})$\\

\chapter{Лекция 6}
\section{Переписать}

\chapter{Лекция 7}
По определению оценка $\hat{\theta}$ называется эффективной оценкой параметра $\theta$, если:\\
1) $\hat{\theta}$ является наименьшей оценкой для теты\\
2) оценка $\hat{\theta}$ обладает наименьшей дисперсией среди всех несмещённых $\theta$

Замечание: иногда говорят не об эффективной вообще точечной оценке, а об оценке, эффективной в некотором классе. Пусть $\Theta$ - некоторый класс несмещённых оценок для параметра $\theta$.\\
По определению оценка $\hat{\theta}$ называется эффективной в классе $\Theta$, если она имеет наименьшую дисперсию среди всех оценок этого класса, т.е. - $(\forall \tilde{\theta})(D\hat{\theta} \leqslant D\tilde{\theta})$.

Пример:\\
Пусть X - случайная величина, обладающая MX = m и DX = $b^2$. Покажем, что оценка $\hat{m_{1}}(\overrightarrow{X}) = \overline{X}$ является эффективной оценкой для m и b в классе линейных оценок.\\

Решение:\\
1) Линейная оценка имеет вид: $\hat{m}(\overline{X}) = \sum\limits_{i = 1}^{n} \lambda_{i} X_{i} = \lambda_{1} X_{1} + ... + \lambda_{n} X_{n} (*)$\\
где $\lambda_{i} \in R, i  = \overline{1,n}$, тогда матожидание линейной оценки (*) :
a) $M[\hat{m}] = \lambda_{1} M X_{1} + ... + \lambda_{n} M X_{n} = \bigg| X_{i} ~ X_{j}, MX = m\bigg| = (\lambda_{1} + ... + \lambda_{n})m$. Так как оценка является несмещённой, то $M[\hat{m}] = m \Rightarrow \sum\limits_{i=1}^{n} = 1$\\
b) Дисперсия оценки (*):\\
$D[\hat{m}] = \sum\limits_{i=1}^{n} \lambda_{i}^{2} DX_{i} = \lambda^{2} \sum_{i=1}^{n} \lambda_{i}^{2}$ - аналогично матожиданию.\\

2) Попробуем подобрать коэффициент $\lambda_{i}, i = \overline{1, n}$, и (*) так, чтобы:\\
\begin{equation}
\begin{cases}
D[\hat{m}] \rightarrow min\\
\sum\limits_{i=1}^{n} \lambda_{i} = 1\\
\end{cases}
\end{equation}

Для этого нужно решить задачу условной оптимизации:\\
\begin{equation}
\begin{cases}
\phi(\lambda_{1} ... \lambda_{n}) = \lambda_{1}^{2} + ... + \lambda_{n}^{2} \rightarrow min\\
\sum\lambda_{i} = 1
\end{cases}
\end{equation}

Запишем функцию Лагранжа:\\
$L(\lambda_{1} ... \lambda_{n}, \mu) = \lambda_{1}^{2} + ... + \lambda_{n}^{2} - \mu\sum\lambda_{i} - \mu$\\

\begin{equation}
\begin{cases}
\frac{dL}{d\lambda_{i}} = 0\\
\frac{dL}{d\mu} = 0\\
\end{cases}
\end{equation}

Следовательно:\\
\begin{equation}
\begin{cases}
\frac{dL}{d\lambda_{i}} = 2\lambda_{i} - \mu = 0\\
\frac{dL}{d\mu} = - (\sum\limits_{i=1}^{n} \lambda_{i} - 1) = 0\\
\end{cases}
\end{equation}

Из n первых уравнений - $\lambda_{i} = \frac{\mu}{2}, i = \overline{1, n}$\\

Покажем, что найденное решение соответствует точке условного минимума целевой функции, таким образом, подставляя $\lambda_{i}$ в * получаем искомую оценку с минимальной дисперсией в классе линейных оценок.\\
$\hat{m}(\overrightarrow{X}) = \frac{1}{n}X_{1} + ... + \frac{1}{n} X_{n} = \overline{X}$\\
Дисперсия этой оценке:\\
$D[\hat{m}] = \sigma^{2}\sum_{i=1}^{n}\lambda_{i}^{2} = \frac{\sigma^{2}}{n}$\\

Теорема:\\
Теорема о единственности эффективной оценки:\\
Пусть $\tilde{\theta_{1}}(\overline{X})$ и $\tilde{\theta_{2}}(\overline{X})$ - эффективные оценки некой оценки параметра $\theta$, тогда $\tilde{\theta_{1}}(\overline{X}) = \tilde{\theta_{2}}(\overline{X})$

\section{Неравенство Рао-Крамера}
Пусть X - случайная величина, закон распределения которой зависит от вектора $\overrightarrow{\theta} = (\theta_{1}, ..., \theta_{n})$ параметров.\\
Пусть $\overrightarrow{X} = (X_{1}, ..., X_{n})$ - случайная выборка из генеральной совокупности X.\\

Опеределение - функцией правдоподобия, отвечающей случайной выборке  $\overrightarrow{X}$ называется функция $L(\overrightarrow{X}, \overrightarrow{\theta}) = p(X_{1}, \overrightarrow{\theta}) ... p(X_{1}, \overrightarrow{\theta})$\\
где:\\
\begin{equation}
p(X_{i}, \overrightarrow{\theta}) = 
\begin{cases}
f(X_{i}, \overrightarrow{\theta}), \text{если X - непрерывная случайная величина}\\
P(X = X_{i}), \text{если X - дискретная случайная величина}\\
\end{cases}
\end{equation}

Пусть r = 1, т.е. $\overrightarrow{\theta} = (\theta_{1}) = (\theta)$\\

Определение - количество информации по Фишеру, содержащееся в случайной величине $\overrightarrow{X}$, называется число $I(\theta) = M[(\frac{d ln L}{d\theta})^{2}]$\\

Теорема:\\
Неравенство Рао-Крамера:\\
Пусть рассматриваемая модель является регулярной, $\hat{\theta}(\overrightarrow{X})$ - несмещённая оценка параметра тета. Тогда:\\
$D[\hat{\theta}] \geqslant \frac{1}{I(\theta)}$ - неравенство Рао-Крамера.\\

Замечание:\\
1) При доказательстве теоремы Рао используются дифференциальные параметры под знаком интеграла:\\
$\frac{d}{d\phi}\int\limits_{G}\phi(\overrightarrow{X}, \theta)dx = \int\limits_{G} \frac{d\phi(\overrightarrow{X}, \theta)}{d\theta}dx$\\
Т.е. параметрические модели, для которых справедливо это равенство, будем называть регулярными. \\
2) Неравенство Рао даёт нижнюю границу для дисперсии для всех возможных оценок параметра $\theta$.\\
3) Величина $e(\hat{\theta}) = \frac{1}{I(\theta)D(\hat{\theta})}$ называется показателем эффективности по Рао точечной оценки $\hat{\theta}$\\
$0 \leqslant e(\hat{\theta}) \leqslant 1$\\
Очевидно, что оценка эффективная по Рао будет ''просто'' эффективной. Вопрос в том, для каких параметричесих моделей существует эффективная по Рао оценка (то есть существует оценка, дисперсия которой равна $\frac{1}{I(\theta)}$) мы оставим без рассмотрения.\\
4) В некоторых случаях вводят в рассмотрение величину $I_{0} (\theta) = M[(\frac{dp(X, \theta)}{d\theta})^{2}]$\\
где $p(X, \theta)$ имеет тот же смысл, что и функция правдоподобия.\\
Данную величину можно назвать количеством информации по Фишеру в одном испытании. Для некоторых параметрических моделей справедливо:\\
$I(\theta) = nI_{0}(\theta)$,\\
где n - объём случайной информации.\\

Пример:\\
Пусть $X ~ N(m, \sigma^{2})$, где m - неизвестна, $\sigma$ - известна. Докажем, что оценка $\hat{m_{1}}(\overrightarrow{X}) = \overline{X}$ для m является эффективной по Рао.\\
1) Необходимо найти показатель эффективности оценки $\hat{m_{1}}$:\\
$e(\hat{m}) = \frac{1}{I(m)D(\hat{m})}$, если данная величина равна 1, то оценка эффективна по Рао, иначе не является эффективной по Рао.
2) $D[\hat{m}] = D[\overline{X}] = ... = \frac{\sigma^{2}}{n}$\\
3) $I(\hat{m}) = ?$\\
$I(\hat{m})=M[(\frac{d ln L}{dn})^{2}]$, составим функцию L правдоподобия:\\
$L(\overline{X}, m) = p(X, m)\cdot ... \cdot p(X_{n}, m) = \frac{1}{(\sqrt{2 \pi})^{n} \sigma^{n}}e^{-\frac{1}{2\sigma^{2}}\sum(x_{i}-m)^{2}}$\\
Тогда:\\
$ln L(\overline{X}, n) = -\frac{n}{2}ln 2\pi - n ln \sigma - \frac{1}{2\sigma^{2}}\sum_{i=1}^{n}(m - X_{i})^{2}$\\
$\frac{dln L(\overline{X}, m)}{dm} = -\frac{2}{2\sigma^{2}}\sum\limits_{i=1}^{n}(m - X_{i}) = \frac{1}{\sigma^{2}}\sum_{i=1}^{n}(X_{i} - m)$\\
$(\frac{dln L(\overline{X}, m)}{dm})^{2} = \frac{1}{\sigma^{4}}[(X_{1} - m) + ... + (X_{n} - m)]^{2}$\\
Т.о.:\\
$I(m) = M[(\frac{d ln L(\overline{X}, m)}{dm})^{2}] = \frac{1}{\sigma^{4}}[\sum\limits_{i=1}^{n}M[(X_{i} - m)^{2} + 2\sum\limits_{1 \leqslant i < j \leqslant n}M[(X_{i} - m)(X_{j} - m)]] = \frac{1}{\sigma^{4}}[\sum\limits_{i=1}^{n} \sigma^{2} + 0] = \frac{1}{\sigma^{4}} n\sigma^{2} = \frac{n}{\sigma^{2}}$\\
4) Получаем $e(\hat{m})$\\

\chapter{Лекция 8}
\section{Методы построения точечных оценок}
1. Метод моментов\\
2. Метод максимального правдоподобия.\\

\section{Метод моментов}
Пусть:\\
1) X - некий случайный вектор, распередление которого зависит от вектора $\overrightarrow{\theta} = (\theta_{1}, ..., \theta_{n})$ неизвестных параметров.\\
2) $\exists r $ первых моментов случайного вектора X, то есть $\exists M[X^{k}], k = 1,.. r$\\

Тогда в методе моментов:\\
1) Вычисляются теоретические моменты 1-го, 2-го, ..., r-го порядков, зависящих от неизвестных параметров:\\
$m_{k}(\theta_{1}, ..., \theta_{r} = M[X^{k}], k = \overline{1, r}$ - теоретические моменты порядка k\\

2) Теоретические моменты приравниваются к выборочным аналогам:\\

\begin{equation}
\begin{cases}
m_{1}(\theta_{1}, ..., \theta_{r}) = \hat{m_{1}}(\overrightarrow{X})\\
...\\
m_{r}(\theta_{1}, ..., \theta_{r}) = \hat{m_{r}}(\overrightarrow{X})\\
\end{cases}
\end{equation}
Система уравнений (возможно, нелинейных), относительно неизвестных параметров тета.\\

3) Решаем получившуюся систему:\\
\begin{equation}
\begin{cases}
\theta_{1} = \hat{\theta_{1}}(\overrightarrow{X})\\
...\\
\theta_{r} = \hat{\theta_{r}}(\overrightarrow{X})\\
\end{cases}
\end{equation}

Полученные зависимости используются в качестве точечных оценок для полуинтервалов.\\

Замечание:\\
Иногда некоторые уравнения системы из подпункта (2) удобнее записывать относительно центральных, а не начальных моментов. В этом случает k-е уравнение будет иметь вид:\\
$\mathring{m_{k}}(\theta_{1}, ..., \theta_{r}) = \hat{\circ{m_{k}}}(\overrightarrow{X})$, где:\\
$\mathring{m_{k}}(\theta_{1}, ..., \theta_{r}) = M[(X - MX)^{k}]$\\
$\hat{\mathring{m_{k}}}(\overrightarrow{X}) = \frac{1}{n}\sum\limits_{i=1}^{n}(X_{i} - \overline{X})^{k}$\\

Пример:
Пусть $X ~ R[a,b]$, гже а, b - произвольные параметры. С помощью метода моментов построить точечные оценки для a, b.\\
1)\\
\begin{equation}
X ~ R[a, b] \Rightarrow f(x) = 
\begin{cases}
\frac{1}{b - a}, x \in [a ,b]\\
0, \text{иначе}\\
\end{cases}
\end{equation}

Неизвестные параметры (a и b), следовательно требуется два уравнения:\\
\begin{equation}
\begin{cases}
m_{1}(a, b) = \hat{m_{1}}(\overrightarrow{X}) \text{ - относительно начального момента 1-го порядка}\\
m_{2}(a, b) = \hat{m_{2}}(\overrightarrow{X}) \text{ - относительно центрального момента второго порядка}\\
\end{cases}
\end{equation}

2) Найдём теоретические моменты:\\
$m_{1}(a,b) = MX = \frac{a + b}{2}$\\
$\circ{m_{2}}(a,b) = DX = \frac{(b - a)^{2}}{12}$\\

Запишем выборочные моменты:\\
$\hat{m_{1}}(a,b)=\frac{1}{n}\sum_{i=1}{n}X_{i} = \overline{X}$\\
$\mathring{m_{2}}(a,b) = \frac{1}{n}\sum_{i=1}{n}(X-\overline{X})^{2} = \hat{\sigma}^{2}(\overrightarrow{X})$\\
или\\
$\mathring{m_{2}}(a,b) = \frac{1}{n - 1}\sum_{i=1}{n}(X-\overline{X})^{2} = S^{2}(\overrightarrow{X})$\\

Используем $S^{2}$\\
3) Приходим к следующей системе уравнений:\\
\begin{equation}
\begin{cases}
\overline{X} = \frac{a + b}{2}\\
S^{2} = \frac{(b - a)^{2}}{12}\\
\end{cases}
\end{equation}
a, b = ?\\
Далее система решается стандартно\\

Замечание:\\
1) Поскольку выбранные моменты $\hat{m_{k}}(\overrightarrow{X})$ и $\hat{\mathring{m_{k}}}(\overrightarrow{X})$ являются состоятельными оценками соотвествтующих теоретических моментов, то можно показать, что в случае непрерывной зависимости решения системы из пункта (2) от $\hat{m_{k}}$ (или от $\mathring{m_{k}}$) оценки параметров, полученные с использованием этого метода также являются состоятельными.\\
2) Так как выборочные моменты степени k при $k \geqslant 2$ являются смещёнными оценками своих теоретических аналогов, то и оценки параметров, полученные с помощью метода моментов также могут быть смещёнными.

\section{Метод максимального правдоподобия}
Пусть:\\
1) X - случайная величина, зависящая от вектора параметров $\overrightarrow{\theta} = (\theta_{1}, ..., \theta_{r})$\\

Ранее было введено понятие функции правдоподобия случайной выборки X:\\
$L(\overrightarrow{X}, \overrightarrow{\theta}) = p(X_{1}, \overrightarrow{\theta}) \cdot ... \cdot p(X_{n}, \overrightarrow{\theta})$\\,
где:\\
\begin{equation}
p(X_{i}, \overrightarrow{\theta}) = 
\begin{cases}
f(X_{i}, \overrightarrow{\theta}), \text{если X - непрерывная случайная величина}\\
P(X = X_{i}), \text{если X - дискретная случайная величина}\\
\end{cases}
\end{equation}

Можно показать, что чем ближе значение вектора $\overrightarrow{\tilde{\theta}}$ к теоретическому значению вектора тета, тем большие значения принимает функция правдоподобия $L(\overrightarrow{x}, \overrightarrow{\theta})$\\

В методе максимального правдоподобия в качестве точечных оценок неизвестного параметра выступают значения, доставляющие максимальное значение функции правдоподобия. Для реализации метода необходимо решить задачу\\
$L(\overrightarrow{X},\overrightarrow{\theta}) \rightarrow max_{\overrightarrow{\theta}}$ ( * )\\
тогда:\\
$\hat{\overrightarrow{\theta}}(\overrightarrow{X}) = arg max_{\overrightarrow{\theta}} L(\overrightarrow{X}, \overrightarrow{\theta})$\\


Замечание:\\
1) Для решения задачи ( * ) можно использовать неоходимые условия дифференциальной функции нескольких переменных:\\
\begin{equation}
\begin{cases}
\frac{\delta L(\overrightarrow{X}, \overrightarrow{\theta})}{\delta \theta_{1}} = 0\\
\frac{\delta L(\overrightarrow{X}, \overrightarrow{\theta})}{\delta \theta_{2}} = 0\\
\end{cases}
\end{equation}
Данные уравнения называются уравнениями правдоподобия\\
2) Функция L является произведением n сомножетелей и работать с ней не всегда удобно. Поэтому часто вместо задачи ( * ) решют эквивалентную задачу ( ** ):\\
$ln L(\overrightarrow{X},\overrightarrow{\theta}) \rightarrow max_{\overrightarrow{\theta}}$\\
т.е.:\\
$\hat{\overrightarrow{\theta}}(\overrightarrow{X}) = arg max_{\overrightarrow{\theta}} ln L(\overrightarrow{X}, \overrightarrow{\theta})$\\
Данная замена эквивалентна, так как ln - монотонная возрастающая функция.\\	

Пример:\\
$X ~ R[a,b]$, a,b - неизвестные параметры, необходимо с помощью метода максимального правдоподобия построить точеные оценки параметров a и b.\\

Решение:\\
1)\\
\begin{equation}
X ~ R[a, b] \Rightarrow f(x) = 
\begin{cases}
\frac{1}{b - a}, x \in [a ,b]\\
0, \text{иначе}\\
\end{cases}
\end{equation}

Выпишем функцию правдоподобия $L(\overrightarrow{X}, a, b) = \text{так как X - непрерывная случайная величина} = f(X_{1}, a, b) \cdot ... \cdot f(X_{n}, a, b) = \frac{1}{b - a} \cdot \frac{1}{b - a} \cdot ... \cdot \frac{1}{b - a}  = \frac{1}{(b - a)^{n}}$\\
2) $ ln L = ln \frac{1}{(b - a)^{n}} = -n ln (b - a) \rightarrow max_{a,b}$\\
Уравнения правдоподобия:\\
\begin{equation}
\begin{cases}
\frac{\delta ln L}{\delta a} = \frac{n}{b - a} = 0\\
\frac{\delta ln L}{\delta b} = -\frac{n}{b - a} = 0\\
\end{cases}
\end{equation}

n = 0 - противоречие, возникло из-за того, что неверно записана формула для L.\\
Правильная запись формулы:\\
\begin{equation}
L(X, a, b) = 
\begin{cases}
\frac{1}{(b - a)^{n}}, if X(1) \geqslant a, X(n) \leqslant b\\
0, \text{иначе}\\
\end{cases}
\end{equation}

Вычисление $\frac{\delta L(\overrightarrow{X}, a, b)}{\delta a}, \frac{\delta L(\overrightarrow{X}, a, b)}{\delta b}$ затруднительно, так как a и b также зависят от области, в которой L > 0\\
Попробуем в ''в лоб'' найти $max_{a,b} L(\overrightarrow{X},a,b)$\\
a) если\\
\begin{equation}
\begin{cases}
a \leqslant X_{(1)}\\
b \geqslant X_{(n)}\\
\end{cases}
\end{equation}
то L > 0, в противном случае L = 0. Так как $L \rightarrow max$, то условия на a, b в любом случае должны быть верны\\
b) при вычислении условий на a, b\\
$L = \frac{1}{(b - a)^{n}}$\\
Так как $L \rightarrow max$, то $b - a \rightarrow min$\\
Тогда приближаем к подзадаче:
\begin{equation}
\begin{cases}
b - a \rightarrow min\\
a \leqslant X_{(1)}\\
b \geqslant X_{(n)}\\
\end{cases}
\end{equation}

Таким образом:
\begin{equation}
\begin{cases}
\hat{a}(\overrightarrow{X}) = X_{(1)}\\
\hat{b}(\overrightarrow{X}) = X_{(n)}\\
\end{cases}
\end{equation}

\section{Интервальные оценки}
Основные понятия:\\
1) Рассматривается вторая задача математической статистики.\\

Ранее для решения этой задачи использовались точечные оценки. Тогда принимались равенства $\theta_{j} = \hat{\theta_{j}}(\overrightarrow{x}), j = \overline{1, k}$\\
Для некоторых статистик $\hat{\theta_{1}}, ..., \hat{\theta_{r}}$. Недостатком данного подходя является то, что он выдаёт информацию о вероятностных характеритиках точности оценивания неизвестных параметров. Кроме точечных оценок для решения второй задачи математической статистики используется другой подход. Для простоты будем считать, что у нас только один неизвестный параметр $r = 1, \overrightarrow{\theta} = (\theta_{1}) = (\theta) = \theta$.

Опеределение:\\
Интервальной оценкой параметра $\theta$ уровня $\gamma$ ($\gamma$-интервальной оценкой) называют пару интервальных статистик $\underline{\theta}(\overrightarrow{X}), \overline{\theta}(\overrightarrow{X})$ таких, что:\\
$P\{\theta \in (\underline{\theta}(\overrightarrow{X}), \overline{\theta}(\overrightarrow{X})\} = \gamma$\\

Замечание:\\
1) Интервальная оценка является интервалом со случайными границами $\underline{\theta}(\overrightarrow{X}), \overline{\theta}(\overrightarrow{X}$, который  накрывает инизвестно теоретическое значение параметра с вероятностью $\gamma$.

































